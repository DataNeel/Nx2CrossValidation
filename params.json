{"name":"Nx2 Cross Validation","tagline":"Nx2-fold Cross Validation for binary classifiers in Python","body":"### Automatically comparing classifier performance\r\nWhen testing binary classifiers, a way to automatically compare the performance of various models and test for statistically significant differences can greatly speed up the decision making process. The following code carries out Nx2-fold cross validation when provided with a dataset and list of models to compare. Undersampling levels can also be provided in order to balance the minority and majority classes. If the winner is not clear from the results, a significance test can be run on the results table to determine whether or not the accuracy measures of different models are significantly different.\r\n\r\n\r\n### Nx2-fold validation\r\nNx2-fold validation splits the data set into two halves, A and B. A is used to train the model, which is then tested on B. This process is then repeated with B as the training set and A as the testing set. This process is repeated N times (often 5), resulting in Nx2 accuracy measures that are all the product of different training and testing data samples. K folds validation, by contrast, often uses very similar training data sets when a large number is used for K.\r\n\r\n### Comparing performance with area under the ROC curve (AUC)\r\nThe ROC curve plots the sensitivity and specificity of a binary classifier as the classification threshold is varied from 0 to 1. An AUC of 1 means that the classifier works perfectly, and randomly classifying samples between the two classes would result in an AUC of .5. Because the AUC is independent of the chosen threshold, it is a good way to measure overall fit in a model where you have the option of picking a threshold to satisfy business rules.\r\n\r\n### Significance testing for differences in performance\r\nDietterich (1998) proposed when several models are trained and tested on the same Nx2 data sets, the accuracy measures can be used to construct an F test for significant differences between the performance of two models. This process is also outlined in Burez and Van den Poel (2009). Both papers are linked to at the bottom of this page.\r\n\r\n### Required libraries\r\n* Pandas\r\n* Numpy\r\n* Scipy for significance testing\r\n* Scikit-learn for classification algorithms\r\n\r\n### The functions\r\n```\r\nevaluate_models(X,y,models,times, undersampling=[0],time_unit='s')\r\n```\r\nThis function takes in the data, models, and parameters and returns a set of Nx2 results for each model. For each level of undersampling, all models are run in parallel.\r\n* X - The feature matrix. Must be provided in 0s and 1s\r\n* y - The target classes\r\n* models - A python list of binary classifier models to be tested. Each model must have a model name assigned to it after being created (e.g. model.name = 'model1')\r\n* undersampling - A list of undersampling proportions to attempt. 0 indicates no undersampling and is used by default\r\n* time_unit - The unit of time to be used for avg. run time and avg. predict time in the results table (s, m, or h)\r\n\r\n```\r\nrank_models(results)\r\n```\r\nWhen provided with the results dataframe that is generated by evaluate_models, rank_models() will return the average statistics of models ranked by ROC AUC in descending order\r\n\r\n```\r\nsignificance(results)\r\n```\r\nWhen provided with the results dataframe, the significance function will generate a matrix that contains rows and columns for each model and undersampling level. Each cell of this matrix represents the probability value that the two models have the same level of accuracy. This probability is calculated using an F test.\r\n\r\n### Example\r\nFirst, we generate a number of models to test and provide them with names.\r\n```\r\n#Create classifiers to test\r\nforest = RandomForestClassifier(n_estimators = 10, n_jobs=2, max_depth=50)\r\nforest.name='forest1'\r\nforest2 = RandomForestClassifier(n_estimators = 50, n_jobs=4, max_depth=100)\r\nforest2.name='forest2'\r\nlogit=LogisticRegression()\r\nlogit.name='logit1'\r\nada= AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),n_estimators = 10)\r\nada.name='ada1'\r\ngforest= GradientBoostingClassifier(n_estimators = 10, max_depth=2, subsample=.5)\r\ngforest.name='gforest1'\r\ngforest2= GradientBoostingClassifier(n_estimators = 50, max_depth=3, subsample=.5)\r\ngforest2.name='gforest2'\r\n```\r\n\r\nNext, an artificial classification dataset with severe class imbalance is generated. 95% of records in this set will belong to class 0 (as specified by the weights parameter).\r\n```\r\n#Generate a fake binary classification dataset with extreme class imbalance\r\nfrom sklearn.datasets import make_classification\r\ndata=make_classification(n_samples=100000, n_features=100, n_informative=4, weights=[.95], flip_y=.02, n_repeated=13, class_sep=.5)\r\nX=pd.DataFrame(data[0])\r\ny=pd.Series(data[1])\r\n```\r\n\r\nThis dataset is run through our list of models with 5x2-fold cross validation and with the majority class undersampled until the minority class accounts for 10% and 20% of the data as well as with no undersampling.\r\n```\r\n#Run the models\r\nresults=evaluate_models(X,y,[logit,forest,forest2,ada,gforest,gforest2],5, undersampling=[0,.1,.2], time_unit='m')\r\n```\r\nThe results are large, but here is a preview...\r\n```\r\n>>> results.head(10)\r\n      model  iteration  fold undersampling       auc  fit_time  predict_time\r\n0    logit1          0     1          none  0.664417  0.015710      0.000430\r\n1    logit1          0     2          none  0.672645  0.017392      0.000408\r\n2   forest1          0     1          none  0.782605  0.097416      0.002430\r\n3   forest1          0     2          none  0.808222  0.095979      0.002295\r\n4   forest2          0     1          none  0.838131  0.245126      0.003866\r\n5   forest2          0     2          none  0.838551  0.232675      0.003725\r\n6      ada1          0     1          none  0.817182  0.346996      0.003207\r\n7      ada1          0     2          none  0.817818  0.320188      0.003003\r\n8  gforest1          0     1          none  0.705441  0.073240      0.000530\r\n9  gforest1          0     2          none  0.719508  0.071568      0.000460\r\n``` \r\nThe rank_models() function then aggregates the data to show which model performed best. Times shown are in minutes.\r\n```\r\n>>> rank_models(results)\r\n                             auc  fit_time  predict_time\r\nmodel    undersampling                                  \r\nforest2  0.2            0.846917  0.048240      0.004250\r\nforest2  0.1            0.845637  0.120340      0.003976\r\nforest2  none           0.839483  0.237978      0.003786\r\ngforest2 0.2            0.825184  0.118848      0.001160\r\ngforest2 0.1            0.821249  0.249924      0.001234\r\nforest1  0.2            0.816427  0.019376      0.002227\r\ngforest2 none           0.815302  0.448876      0.001377\r\nforest1  0.1            0.808853  0.048392      0.002267\r\nada1     none           0.808564  0.335405      0.003131\r\nada1     0.1            0.805994  0.189129      0.003135\r\nada1     0.2            0.804026  0.089088      0.003203\r\nforest1  none           0.795091  0.096185      0.002295\r\ngforest1 0.2            0.734390  0.018959      0.000504\r\ngforest1 0.1            0.724401  0.039700      0.000483\r\ngforest1 none           0.714376  0.071366      0.000470\r\nlogit1   0.2            0.674707  0.003152      0.000414\r\nlogit1   0.1            0.671341  0.008916      0.000391\r\nlogit1   none           0.669132  0.015787      0.000413\r\n```\r\n\r\nThe significance results are also large, but here are the results for the gforest2 and forest2 models. High values indicate that the two models are not significantly different.\r\n```\r\n>>> significance(results)[['gforest2','forest2']]\r\nModel                       gforest2      gforest2      gforest2  \\\r\nUndersampling                   none           0.2           0.1\r\nModel    Undersampling\r\nada1     none              0.5381128     0.1590714     0.3717211\r\nada1     0.2             0.007221477   0.002052937  9.122191e-05\r\nada1     0.1               0.3916473    0.04004108    0.03473235\r\ngforest2 none                    NaN    0.01308544    0.09802531\r\ngforest2 0.2              0.01308544           NaN     0.1741726\r\ngforest2 0.1              0.09802531     0.1741726           NaN\r\nforest2  none            0.002961209   0.006890251    0.01451488\r\nforest2  0.2            0.0002602179  2.377107e-06  0.0005662108\r\nforest2  0.1            0.0001374467  2.365901e-05  0.0003685025\r\nforest1  none              0.0483831   0.008635023    0.02564205\r\nforest1  0.2               0.5963419    0.07934876      0.541395\r\nforest1  0.1               0.2102436   0.004488396    0.03481045\r\ngforest1 none           8.510569e-06  8.251625e-06  8.129192e-06\r\ngforest1 0.2            2.620663e-06  7.024162e-06  7.280183e-06\r\ngforest1 0.1            4.977668e-05  0.0001022563  0.0001428671\r\nlogit1   none           8.258857e-07  4.268434e-07  1.153785e-07\r\nlogit1   0.2            8.324898e-07  5.092532e-07  1.357096e-07\r\nlogit1   0.1            9.334062e-07  4.178198e-07  1.478798e-07\r\n\r\nModel                        forest2       forest2       forest2\r\nUndersampling                   none           0.2           0.1\r\nModel    Undersampling\r\nada1     none             0.01016006    0.01025593   0.008048507\r\nada1     0.2            0.0009343802  0.0001625981   6.61957e-05\r\nada1     0.1              0.01351534   0.002296429   0.002967971\r\ngforest2 none            0.002961209  0.0002602179  0.0001374467\r\ngforest2 0.2             0.006890251  2.377107e-06  2.365901e-05\r\ngforest2 0.1              0.01451488  0.0005662108  0.0003685025\r\nforest2  none                    NaN    0.03965544    0.04566424\r\nforest2  0.2              0.03965544           NaN     0.3502626\r\nforest2  0.1              0.04566424     0.3502626           NaN\r\nforest1  none            0.001257939  0.0006595105  0.0005784847\r\nforest1  0.2             0.000312439  0.0002433438  0.0004074213\r\nforest1  0.1            5.765322e-05  4.521528e-05  1.641226e-05\r\ngforest1 none           8.637735e-06   5.23887e-06  2.411338e-06\r\ngforest1 0.2            2.073483e-06  3.211817e-06  9.978427e-07\r\ngforest1 0.1            9.419857e-05  5.354328e-05  4.748392e-05\r\nlogit1   none           8.117545e-07  2.211232e-07  1.839379e-07\r\nlogit1   0.2            9.489241e-07  2.667403e-07  2.066221e-07\r\nlogit1   0.1            7.953832e-07  2.061502e-07  1.806246e-07\r\n```\r\n\r\n\r\n### Further reading\r\n* Handling Class Imbalance in Customer Churn Prediction - Burez and Van den Poel (2009): http://scholar.google.com/scholar?cluster=9767278684451707768&hl=en&as_sdt=0,11\r\n* Approximate statistical tests for comparing supervised classification learning algorithms - Dietterich (1998): http://scholar.google.com/scholar?cluster=1634956806564791154&hl=en&as_sdt=0,11&as_vis=1\r\n* Applied Predictive Modeling Chapter 16 - Kuhn and Johnson: http://appliedpredictivemodeling.com/\r\n* ROC Curve: http://en.wikipedia.org/wiki/Receiver_operating_characteristic","google":"UA-53415288-1","note":"Don't delete this file! It's used internally to help with page regeneration."}